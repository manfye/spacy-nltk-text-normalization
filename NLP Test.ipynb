{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "Example_Sentence = \"Patients who in late middle age have smoked 20 cigarettes a day since their teens constitute an at-risk group. One thing they’re clearly at risk for is the acute sense of guilt that a clinician can incite, which immediately makes a consultation tense.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_process(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    #Tokenization and lemmatization are done with the spacy nlp pipeline commands\n",
    "    lemma_list = []\n",
    "    for token in doc:\n",
    "        lemma_list.append(token.lemma_)\n",
    "    print(\"Tokenize+Lemmatize:\")\n",
    "    print(lemma_list)\n",
    "    \n",
    "    #Filter the stopword\n",
    "    filtered_sentence =[] \n",
    "    for word in lemma_list:\n",
    "        lexeme = nlp.vocab[word]\n",
    "        if lexeme.is_stop == False:\n",
    "            filtered_sentence.append(word) \n",
    "    \n",
    "    #Remove punctuation\n",
    "    punctuations=\"?:!.,;\"\n",
    "    for word in filtered_sentence:\n",
    "        if word in punctuations:\n",
    "            filtered_sentence.remove(word)\n",
    "    print(\" \")\n",
    "    print(\"Remove stopword & punctuation: \")\n",
    "    print(filtered_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "p_stemmer = PorterStemmer()\n",
    "#s_stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "def nltk_process(text):\n",
    "    #Tokenization\n",
    "    nltk_tokenList = word_tokenize(text)\n",
    "    \n",
    "    #Stemming\n",
    "    nltk_stemedList = []\n",
    "    for word in nltk_tokenList:\n",
    "        nltk_stemedList.append(p_stemmer.stem(word))\n",
    "        #nltk_stemedList.append(s_stemmer.stem(word))\n",
    "    \n",
    "    #Lemmatization\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    nltk_lemmaList = []\n",
    "    for word in nltk_stemedList:\n",
    "        nltk_lemmaList.append(wordnet_lemmatizer.lemmatize(word))\n",
    "    \n",
    "    print(\"Stemming + Lemmatization\")\n",
    "    print(nltk_lemmaList)\n",
    "\n",
    "    #Filter stopword\n",
    "    filtered_sentence = []  \n",
    "    nltk_stop_words = set(stopwords.words(\"english\"))\n",
    "    for w in nltk_lemmaList:  \n",
    "        if w not in nltk_stop_words:  \n",
    "            filtered_sentence.append(w)  \n",
    "\n",
    "    #Removing Punctuation\n",
    "    punctuations=\"?:!.,;\"\n",
    "    for word in filtered_sentence:\n",
    "        if word in punctuations:\n",
    "            filtered_sentence.remove(word)\n",
    "    print(\" \")\n",
    "    print(\"Remove stopword & Punctuation\")\n",
    "    print(filtered_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming + Lemmatization\n",
      "['patient', 'who', 'in', 'late', 'middl', 'age', 'have', 'smoke', '20', 'cigarett', 'a', 'day', 'sinc', 'their', 'teen', 'constitut', 'an', 'at-risk', 'group', '.', 'one', 'thing', 'they', '’', 're', 'clearli', 'at', 'risk', 'for', 'is', 'the', 'acut', 'sen', 'of', 'guilt', 'that', 'a', 'clinician', 'can', 'incit', ',', 'which', 'immedi', 'make', 'a', 'consult', 'ten', '.']\n",
      " \n",
      "Remove stopword & Punctuation\n",
      "['patient', 'late', 'middl', 'age', 'smoke', '20', 'cigarett', 'day', 'sinc', 'teen', 'constitut', 'at-risk', 'group', 'one', 'thing', '’', 'clearli', 'risk', 'acut', 'sen', 'guilt', 'clinician', 'incit', 'immedi', 'make', 'consult', 'ten']\n",
      "Wall time: 1.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nltk_process(Example_Sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize+Lemmatize:\n",
      "['patient', 'who', 'in', 'late', 'middle', 'age', 'have', 'smoke', '20', 'cigarette', 'a', 'day', 'since', '-PRON-', 'teen', 'constitute', 'an', 'at', '-', 'risk', 'group', '.', 'one', 'thing', '-PRON-', 'be', 'clearly', 'at', 'risk', 'for', 'be', 'the', 'acute', 'sense', 'of', 'guilt', 'that', 'a', 'clinician', 'can', 'incite', ',', 'which', 'immediately', 'make', 'a', 'consultation', 'tense', '.']\n",
      " \n",
      "Remove stopword & punctuation: \n",
      "['patient', 'late', 'middle', 'age', 'smoke', '20', 'cigarette', 'day', '-PRON-', 'teen', 'constitute', '-', 'risk', 'group', 'thing', '-PRON-', 'clearly', 'risk', 'acute', 'sense', 'guilt', 'clinician', 'incite', 'immediately', 'consultation', 'tense']\n",
      "Wall time: 15 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spacy_process(Example_Sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About spaCy's custom pronoun lemma for English\n",
    "spaCy adds a special case for English pronouns: all English pronouns are lemmatized to the special token -PRON-. Unlike verbs and common nouns, there’s no clear base form of a personal pronoun. Should the lemma of “me” be “I”, or should we normalize person as well, giving “it” — or maybe “he”? spaCy’s solution is to introduce a novel symbol, -PRON-, which is used as the lemma for all personal pronouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Own Reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patients NOUN ROOT patient\n",
      "who PRON nsubj who\n",
      "in ADP prep in\n",
      "late ADJ amod late\n",
      "middle ADJ amod middle\n",
      "age NOUN pobj age\n",
      "have AUX aux have\n",
      "smoked VERB relcl smoke\n",
      "20 NUM nummod 20\n",
      "cigarettes NOUN dobj cigarette\n",
      "a DET det a\n",
      "day NOUN npadvmod day\n",
      "since SCONJ mark since\n",
      "their DET poss -PRON-\n",
      "teens NOUN nsubj teen\n",
      "constitute VERB advcl constitute\n",
      "an DET det an\n",
      "at ADP nmod at\n",
      "- PUNCT punct -\n",
      "risk NOUN pobj risk\n",
      "group NOUN dobj group\n",
      ". PUNCT punct .\n",
      "One NUM nummod one\n",
      "thing NOUN npadvmod thing\n",
      "they PRON nsubj -PRON-\n",
      "’re VERB csubj be\n",
      "clearly ADV advmod clearly\n",
      "at ADP prep at\n",
      "risk NOUN pobj risk\n",
      "for ADP prep for\n",
      "is AUX ROOT be\n",
      "the DET det the\n",
      "acute ADJ amod acute\n",
      "sense NOUN attr sense\n",
      "of ADP prep of\n",
      "guilt NOUN pobj guilt\n",
      "that SCONJ mark that\n",
      "a DET det a\n",
      "clinician NOUN nsubj clinician\n",
      "can VERB aux can\n",
      "incite VERB relcl incite\n",
      ", PUNCT punct ,\n",
      "which DET nsubj which\n",
      "immediately ADV advmod immediately\n",
      "makes VERB relcl make\n",
      "a DET det a\n",
      "consultation NOUN nsubj consultation\n",
      "tense ADJ ccomp tense\n",
      ". PUNCT punct .\n",
      "Wall time: 3.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Import spaCy and load the language library\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(Example_Sentence)\n",
    "\n",
    "# Print each token separately\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wherein', 'used', '’ve', 'call', 'since', 'so', 'upon', 'onto', 'whereby', 'first', 'four', 'herself', 'beforehand', \"'d\", 'i', 'becoming', 'show', 'even', 'ever', 'same', 'always', 'top', 'and', 'meanwhile', 'been', 'another', 'rather', 'seems', 'that', 'whereas', 'these', 'somewhere', 'only', 'mine', 'amount', 'other', 'you', 'never', 'due', 'keep', 'does', 'be', 'nor', 'seeming', 'he', 'former', 'put', 'others', 'ours', 'were', '’re', 'than', 'although', 'less', 'against', 'afterwards', \"'m\", 'quite', 'regarding', 'thereafter', 'what', 'toward', 'nine', 'however', 'whose', 'us', 'his', 'fifty', 'thence', 'fifteen', 'eight', 'across', 'move', 'twelve', 'almost', 'thereupon', '’ll', 'me', 'nevertheless', 'up', 'would', 'hereupon', 'whoever', 'during', 'whenever', 'then', '’m', 'below', '’d', 'we', 'did', 'if', 'why', 'here', 'again', 'nowhere', 'seemed', 'amongst', 'further', 'along', 'otherwise', 'somehow', 'she', '‘m', 'but', 'noone', 'one', 'between', 'indeed', 'must', 'neither', 'name', 'two', 'off', 'everything', 'its', 'where', 'whither', 'too', 'mostly', 'under', 'nobody', 'side', 'behind', 'once', 'the', '’s', 'whence', 'cannot', 'whatever', 'else', '‘s', 'thus', 'beyond', 'latter', 'someone', 'seem', 'doing', 'hence', 'yours', 'very', 'front', 'some', 'no', 'myself', 'is', 'out', 'into', 'hers', 'your', 'say', 'every', 'just', 'often', 'such', 'beside', 'thru', 'ca', 'to', 'serious', 'formerly', 'have', \"'ll\", 'via', 'both', 'elsewhere', 'has', 'this', 'bottom', 'an', 'anyhow', 'become', 'among', 'go', 'them', 'everywhere', 'him', 'twenty', 'may', 'whereupon', 'her', 'through', 'n‘t', 'forty', 'around', 'still', 'latterly', 're', 'themselves', 'per', \"'re\", 'hundred', 'all', 'will', 'towards', 'also', 'part', 'any', 'after', 'now', 'from', '‘d', 'as', 'though', 'already', 'everyone', 'alone', 'really', 'had', 'perhaps', 'therefore', 'empty', 'five', 'herein', 'can', 'hereby', 'within', 'next', 'few', 'or', 'anyway', 'by', 'own', 'their', 'could', 'either', 'those', 'well', 'ourselves', 'above', 'being', 'together', 'down', 'namely', \"'ve\", 'of', 'our', 'none', 'with', 'back', 'except', 'do', 'various', 'third', 'each', 'at', 'anywhere', 'last', 'whether', 'himself', 'before', 'please', 'something', 'make', 'yet', 'when', '‘ll', 'much', 'might', 'made', 'several', 'became', 'wherever', '‘re', 'how', 'yourself', 'sometime', 'without', 'full', 'give', 'enough', 'throughout', 'whereafter', 'six', 'unless', 'eleven', 'was', 'ten', 'more', 'a', 'there', 'take', 'done', 'moreover', 'until', \"n't\", \"'s\", 'sixty', 'anyone', '‘ve', 'becomes', 'n’t', 'anything', 'most', 'thereby', 'should', 'about', 'are', 'while', 'many', 'on', 'they', 'not', 'whole', 'whom', 'least', 'three', 'nothing', 'which', 'sometimes', 'see', 'itself', 'therein', 'who', 'using', 'yourselves', 'hereafter', 'it', 'besides', 'in', 'am', 'over', 'because', 'for', 'my', 'get'}\n",
      "326\n"
     ]
    }
   ],
   "source": [
    "print(nlp.Defaults.stop_words)\n",
    "print(len(nlp.Defaults.stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Patients', 'who', 'in', 'late', 'middle', 'age', 'have', 'smoked', '20', 'cigarettes', 'a', 'day', 'since', 'their', 'teens', 'constitute', 'an', 'at', '-', 'risk', 'group', '.', 'One', 'thing', 'they', '’re', 'clearly', 'at', 'risk', 'for', 'is', 'the', 'acute', 'sense', 'of', 'guilt', 'that', 'a', 'clinician', 'can', 'incite', ',', 'which', 'immediately', 'makes', 'a', 'consultation', 'tense', '.']\n"
     ]
    }
   ],
   "source": [
    "#Tokenizer\n",
    "token_listSC = []\n",
    "for token in doc:\n",
    "    token_listSC.append(token.text)\n",
    "\n",
    "print(token_listSC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['patient', 'who', 'in', 'late', 'middle', 'age', 'have', 'smoke', '20', 'cigarette', 'a', 'day', 'since', '-PRON-', 'teen', 'constitute', 'an', 'at', '-', 'risk', 'group', '.', 'one', 'thing', '-PRON-', 'be', 'clearly', 'at', 'risk', 'for', 'be', 'the', 'acute', 'sense', 'of', 'guilt', 'that', 'a', 'clinician', 'can', 'incite', ',', 'which', 'immediately', 'make', 'a', 'consultation', 'tense', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lemma_list = []\n",
    "for token in doc:\n",
    "        lemma_list.append(token.lemma_)\n",
    "\n",
    "print(lemma_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['patient', 'who', 'in', 'late', 'middle', 'age', 'have', 'smoke', '20', 'cigarette', 'a', 'day', 'since', '-PRON-', 'teen', 'constitute', 'an', 'at', '-', 'risk', 'group', '.', 'one', 'thing', '-PRON-', 'be', 'clearly', 'at', 'risk', 'for', 'be', 'the', 'acute', 'sense', 'of', 'guilt', 'that', 'a', 'clinician', 'can', 'incite', ',', 'which', 'immediately', 'make', 'a', 'consultation', 'tense', '.']\n",
      "['patient', 'late', 'middle', 'age', 'smoke', '20', 'cigarette', 'day', '-PRON-', 'teen', 'constitute', '-', 'risk', 'group', '.', 'thing', '-PRON-', 'clearly', 'risk', 'acute', 'sense', 'guilt', 'clinician', 'incite', ',', 'immediately', 'consultation', 'tense', '.']\n"
     ]
    }
   ],
   "source": [
    "# Create list of word tokens after removing stopwords\n",
    "filtered_sentence =[] \n",
    "\n",
    "for word in lemma_list:\n",
    "    lexeme = nlp.vocab[word]\n",
    "    if lexeme.is_stop == False:\n",
    "        filtered_sentence.append(word) \n",
    "print(lemma_list)\n",
    "print(filtered_sentence)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['patient', 'late', 'middle', 'age', 'smoke', '20', 'cigarette', 'day', '-PRON-', 'teen', 'constitute', '-', 'risk', 'group', 'thing', '-PRON-', 'clearly', 'risk', 'acute', 'sense', 'guilt', 'clinician', 'incite', 'immediately', 'consultation', 'tense']\n"
     ]
    }
   ],
   "source": [
    "#Removing Punctuation\n",
    "punctuations=\"?:!.,;\"\n",
    "for word in filtered_sentence:\n",
    "    if word in punctuations:\n",
    "        filtered_sentence.remove(word)\n",
    "        \n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_stemmer = PorterStemmer()\n",
    "#s_stemmer = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_tokenList = word_tokenize(Example_Sentence) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['patient', 'who', 'in', 'late', 'middl', 'age', 'have', 'smoke', '20', 'cigarett', 'a', 'day', 'sinc', 'their', 'teen', 'constitut', 'an', 'at-risk', 'group', '.', 'one', 'thing', 'they', '’', 're', 'clearli', 'at', 'risk', 'for', 'is', 'the', 'acut', 'sens', 'of', 'guilt', 'that', 'a', 'clinician', 'can', 'incit', ',', 'which', 'immedi', 'make', 'a', 'consult', 'tens', '.']\n"
     ]
    }
   ],
   "source": [
    "nltk_stemedList = []\n",
    "for word in nltk_tokenList:\n",
    "    nltk_stemedList.append(p_stemmer.stem(word))\n",
    "    #nltk_stemedList.append(s_stemmer.stem(word))\n",
    "\n",
    "print(nltk_stemedList)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['patient', 'who', 'in', 'late', 'middl', 'age', 'have', 'smoke', '20', 'cigarett', 'a', 'day', 'sinc', 'their', 'teen', 'constitut', 'an', 'at-risk', 'group', '.', 'one', 'thing', 'they', '’', 're', 'clearli', 'at', 'risk', 'for', 'is', 'the', 'acut', 'sen', 'of', 'guilt', 'that', 'a', 'clinician', 'can', 'incit', ',', 'which', 'immedi', 'make', 'a', 'consult', 'ten', '.']\n"
     ]
    }
   ],
   "source": [
    "nltk_lemmaList = []\n",
    "for word in nltk_stemedList:\n",
    "    nltk_lemmaList.append(wordnet_lemmatizer.lemmatize(word))\n",
    "\n",
    "print(nltk_lemmaList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['patient', 'late', 'middl', 'age', 'smoke', '20', 'cigarett', 'day', 'sinc', 'teen', 'constitut', 'at-risk', 'group', '.', 'one', 'thing', '’', 'clearli', 'risk', 'acut', 'sen', 'guilt', 'clinician', 'incit', ',', 'immedi', 'make', 'consult', 'ten', '.']\n"
     ]
    }
   ],
   "source": [
    "filtered_sentence = []  \n",
    "\n",
    "nltk_stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "for w in nltk_lemmaList:  \n",
    "    if w not in nltk_stop_words:  \n",
    "        filtered_sentence.append(w)  \n",
    "\n",
    "print(filtered_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['patient', 'late', 'middl', 'age', 'smoke', '20', 'cigarett', 'day', 'sinc', 'teen', 'constitut', 'at-risk', 'group', 'one', 'thing', '’', 'clearli', 'risk', 'acut', 'sen', 'guilt', 'clinician', 'incit', 'immedi', 'make', 'consult', 'ten']\n"
     ]
    }
   ],
   "source": [
    "#Removing Punctuation\n",
    "punctuations=\"?:!.,;\"\n",
    "for word in filtered_sentence:\n",
    "    if word in punctuations:\n",
    "        filtered_sentence.remove(word)\n",
    "\n",
    "print(filtered_sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SK-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "stopwordsSK = text.ENGLISH_STOP_WORDS\n",
    "len(stopwordsSK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', 'amount', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around', 'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'bill', 'both', 'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co', 'con', 'could', 'couldnt', 'cry', 'de', 'describe', 'detail', 'do', 'done', 'down', 'due', 'during', 'each', 'eg', 'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', 'few', 'fifteen', 'fifty', 'fill', 'find', 'fire', 'first', 'five', 'for', 'former', 'formerly', 'forty', 'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had', 'has', 'hasnt', 'have', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed', 'interest', 'into', 'is', 'it', 'its', 'itself', 'keep', 'last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made', 'many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'much', 'must', 'my', 'myself', 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine', 'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'part', 'per', 'perhaps', 'please', 'put', 'rather', 're', 'same', 'see', 'seem', 'seemed', 'seeming', 'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'sincere', 'six', 'sixty', 'so', 'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'system', 'take', 'ten', 'than', 'that', 'the', 'their', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'thick', 'thin', 'third', 'this', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward', 'towards', 'twelve', 'twenty', 'two', 'un', 'under', 'until', 'up', 'upon', 'us', 'very', 'via', 'was', 'we', 'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(list(stopwordsSK)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tesla', 'is', 'looking', 'at', 'buying', 'U.S.', 'startup', 'for', '$', '6', 'million', '.']\n",
      "['Tesla', 'looking', 'buying', 'U.S.', 'startup', '$', '6', 'million', '.']\n"
     ]
    }
   ],
   "source": [
    "filtered_sentence = []  \n",
    "\n",
    "word_tokens = word_tokenize(Example_Sentence)  \n",
    "\n",
    "\n",
    "for w in word_tokens:  \n",
    "    if w not in stopwordsSK:  \n",
    "        filtered_sentence.append(w)  \n",
    "\n",
    "print(word_tokens)  \n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
